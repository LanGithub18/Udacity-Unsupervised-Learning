{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lesson-1-Clustering\" data-toc-modified-id=\"Lesson-1-Clustering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lesson 1 Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Types-of-Unsupervised-Learning\" data-toc-modified-id=\"Types-of-Unsupervised-Learning-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Types of Unsupervised Learning</a></span></li><li><span><a href=\"#K-means-algorithm\" data-toc-modified-id=\"K-means-algorithm-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>K-means algorithm</a></span></li><li><span><a href=\"#Choosing-K\" data-toc-modified-id=\"Choosing-K-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Choosing K</a></span></li><li><span><a href=\"#Feature-scaling\" data-toc-modified-id=\"Feature-scaling-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Feature scaling</a></span></li><li><span><a href=\"#Concerns-with-K-means\" data-toc-modified-id=\"Concerns-with-K-means-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Concerns with K-means</a></span></li></ul></li><li><span><a href=\"#Lesson-2-Hiearchical-and-Density-Based-Clustering\" data-toc-modified-id=\"Lesson-2-Hiearchical-and-Density-Based-Clustering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Lesson 2 Hiearchical and Density Based Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hiearchical-clustering\" data-toc-modified-id=\"Hiearchical-clustering-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Hiearchical clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-link-clustering\" data-toc-modified-id=\"Single-link-clustering-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Single link clustering</a></span></li><li><span><a href=\"#Other-hiearchical-clustering\" data-toc-modified-id=\"Other-hiearchical-clustering-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Other hiearchical clustering</a></span></li></ul></li><li><span><a href=\"#Density-Clustering-(DBSCAN)\" data-toc-modified-id=\"Density-Clustering-(DBSCAN)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Density Clustering (DBSCAN)</a></span></li></ul></li><li><span><a href=\"#Lesson-3-Gaussian-Mixture-model-clustering\" data-toc-modified-id=\"Lesson-3-Gaussian-Mixture-model-clustering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Lesson 3 Gaussian Mixture model clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gaussian-Mixture-Model-(GMM)-Clustering\" data-toc-modified-id=\"Gaussian-Mixture-Model-(GMM)-Clustering-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Gaussian Mixture Model (GMM) Clustering</a></span></li><li><span><a href=\"#Expectation-Maximization-(EM)-Algorithm\" data-toc-modified-id=\"Expectation-Maximization-(EM)-Algorithm-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Expectation Maximization (EM) Algorithm</a></span></li><li><span><a href=\"#Applications\" data-toc-modified-id=\"Applications-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Applications</a></span></li><li><span><a href=\"#Cluster-Analysis\" data-toc-modified-id=\"Cluster-Analysis-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Cluster Analysis</a></span></li><li><span><a href=\"#Cluster-validation\" data-toc-modified-id=\"Cluster-validation-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Cluster validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#External-indices\" data-toc-modified-id=\"External-indices-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>External indices</a></span></li><li><span><a href=\"#Internal-indices\" data-toc-modified-id=\"Internal-indices-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Internal indices</a></span></li></ul></li></ul></li><li><span><a href=\"#Lesson-4-Dimension-reduction-and-Principle-component-analysis-(PCA)\" data-toc-modified-id=\"Lesson-4-Dimension-reduction-and-Principle-component-analysis-(PCA)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Lesson 4 Dimension reduction and Principle component analysis (PCA)</a></span></li><li><span><a href=\"#Lesson-5--Random-Projection-and-ICA\" data-toc-modified-id=\"Lesson-5--Random-Projection-and-ICA-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Lesson 5  Random Projection and ICA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Projection\" data-toc-modified-id=\"Random-Projection-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Random Projection</a></span></li><li><span><a href=\"#Independent-Component-Analysis\" data-toc-modified-id=\"Independent-Component-Analysis-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Independent Component Analysis</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Unsupervised Learning\n",
    "\n",
    "1. Clustering: group data based on similarities\n",
    "2. Dimension Reduction: reduce the dimension of large number of features to smaller ones.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means algorithm\n",
    "\n",
    "- Application\n",
    "    book, movie, music, groups of customers\n",
    "- K means the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K\n",
    "\n",
    "It can be determined by different methods.\n",
    "\n",
    "1. Plot the data and observe directly\n",
    "2. Prior knowledge and set K \n",
    "3. Elbow Method\n",
    "    - It is useful when no knowledge before the data is collected. \n",
    "    - Exam the average distance from the K centers while changing K.\n",
    "    - Choose the K at the *elbow*\n",
    "    \n",
    "`KMeans(2).fit(data).score(data)`\n",
    "`KMeans(2).fit(data).predict(data)`\n",
    "\n",
    "**It is possible to have different clustering decision with the same number of centroids.**\n",
    "\n",
    "1. Randomly assign the centroids\n",
    "2. Assign points to the nearest centroid\n",
    "3. Move centroid nearer to the points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "1. Standardize: z-score scaling\n",
    "   - Used in clustering algorithm\n",
    "\n",
    "2. Normalizing: min-max scaling\n",
    "   - Scale the color data of image\n",
    "   \n",
    "   \n",
    "**The scales of the features will affect the clustering decision**\n",
    "\n",
    "**With any distance based machine learning model (regularized regression methods, neural networks, and now kmeans), you will want to scale your data **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from sklearn import preprocessing as p`\n",
    "\n",
    "`df_ss = p.StandardScaler().fit_transform(df)`\n",
    "\n",
    "`df_ss = pd.DataFrame(df_ss) `\n",
    "\n",
    "`df_ss.columns = ['height', 'weight']`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concerns with K-means\n",
    "\n",
    "1. The random placement of the centroids may lead to non-optimal solutions\n",
    "\n",
    "    **Use repeated runs to protect against local minima.**\n",
    "    \n",
    "    \n",
    "2. Scales of the data may affect the clustering decision\n",
    "    **Feature scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 Hiearchical and Density Based Clustering\n",
    "\n",
    "**Conerns about K-means**\n",
    "<img src=\"ConcersAboutKmeans.png\">\n",
    "It is the sideeffect of the K-means relying on the distance as the measure.\n",
    "\n",
    "**Solution**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiearchical clustering\n",
    "\n",
    "### Single link clustering\n",
    "\n",
    "1. Take each point as a single cluster\n",
    "2. Connect the nearest two points and create the first cluster\n",
    "3. Connect other nearest two points and create clusters for them\n",
    "4. Add new point into a exsiting cluster whose closet point to the cluster is the smallest one.\n",
    "5. Repeat this process, the hiearchical trees will cover all the points\n",
    "6. This is called the Dendrogram\n",
    "7. Find the clusters by cutting the tree.\n",
    "\n",
    "*Comparison with K-means*\n",
    "\n",
    "**K-means**: uniformly distributed data; Some clusters with different density but still kind of uniformly distributed; well separated data set\n",
    "\n",
    "**Single-link**: two crescents; two rings; well separated data set\n",
    "\n",
    "**Dendrogram is valuable** to visualize the clustering in the data set especially if the data is in high dimensions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other hiearchical clustering\n",
    "\n",
    "Agglomerative clustering \n",
    "\n",
    "Distance measures are different.\n",
    "\n",
    "1. Complete link clustering\n",
    "At step 4 in the single link clustering, check the farthest point inside the existing clusters to determine the cluster to join.\n",
    "\n",
    "2. Average link clustering\n",
    "Averaging the distances.\n",
    "3. Ward's Method (default method in scikitlearn)\n",
    "Find a central point between two clusters.\n",
    "Add square of distance between every point to the center point and minus the sum of the square of the distance between point in one cluster and the central point inside this cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, cluster\n",
    "\n",
    "x = datasets.load_iris().data[:10]\n",
    "\n",
    "clust = cluster.AgglomerativeClustering(n_clusters = 3, linkage = 'ward')\n",
    "\n",
    "#Note the linkage can be : 'complete', 'average' too\n",
    "labels = clust.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASUElEQVR4nO3de5DdZX3H8ffHBKWKiJpVLKChNVipl7Vu0alj3Y63QKdSR6aTxEu14s5oY8d6GelUkaIzVp2pV7xshUaxK0VrNbVROlNNrVosS11FUJwIKitmWBXxDkK//eOc6Lpsck6Sc0ke3q+ZMzm/3+/J7/kmu/s5z3nO7/ltqgpJ0uHvTuMuQJI0GAa6JDXCQJekRhjoktQIA12SGrF2XB2vW7eu1q9fP67uJemwdPnll3+nqiZWOza2QF+/fj3z8/Pj6l6SDktJvrG3Y065SFIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhoxtoVF2rfZWZibG3cV0v7bsgVmZsZdxR2TI/RD1NwcLCyMuwpp/ywsOBAZJ0foh7DJSdi5c9xVSP2bnh53BXdsjtAlqREGuiQ1omegJ7kgyQ1JvtSj3e8muS3JGYMrT5LUr35G6NuAjftqkGQN8DrgkgHUJEk6AD0Dvao+BXyvR7MXAv8M3DCIoiRJ+++g59CTHAc8FXhnH21nkswnmV9aWjrYriVJywziQ9E3AS+vqtt6Nayq2aqaqqqpiYlVf4OSJOkADeI69CngoiQA64DTktxaVR8ewLklSX066ECvqhP3PE+yDfioYS5Jo9cz0JO8H5gG1iVZBF4FHAFQVT3nzSVJo9Ez0Ktqc78nq6pnH1Q1kqQD5kpRSWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRE9Az3JBUluSPKlvRx/epIvdh+fTfLwwZcpSeqlnxH6NmDjPo5fCzyuqh4GvBqYHUBdkqT9tLZXg6r6VJL1+zj+2WWblwLHH3xZkqT9Neg59OcCH9vbwSQzSeaTzC8tLQ24a0m6YxtYoCf5AzqB/vK9tamq2aqaqqqpiYmJQXUtSaKPKZd+JHkY8G7g1Kr67iDOKUnaPwc9Qk9yf+BDwDOr6qsHX5Ik6UD0HKEneT8wDaxLsgi8CjgCoKreCZwN3Bt4exKAW6tqalgFS5JW189VLpt7HD8TOHNgFUmSDogrRSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IiegZ7kgiQ3JPnSXo4nyVuS7EryxSS/M/gyJUm99DNC3wZs3MfxU4EN3ccM8I6DL0uStL96BnpVfQr43j6anA68tzouBY5Jcr9BFShJ6s8g5tCPA65btr3Y3Xc7SWaSzCeZX1paGkDXkqQ9BhHoWWVfrdawqmaraqqqpiYmJgbQtSRpj0EE+iJwwrLt44HrB3BeSdJ+GESgbwee1b3a5dHATVX17QGcV5K0H9b2apDk/cA0sC7JIvAq4AiAqnonsAM4DdgF/AR4zrCKlSTtXc9Ar6rNPY4X8OcDq0iSdEBcKSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1oq9AT7IxydVJdiU5a5Xj90/yySSfT/LFJKcNvlRJ0r70DPQka4DzgFOBk4HNSU5e0ewVwMVV9QhgE/D2QRcqSdq3fkbopwC7quqaqroFuAg4fUWbAo7uPr8HcP3gSpQk9WNtH22OA65btr0IPGpFm3OAf0/yQuBuwBMGUp0kqW/9jNCzyr5asb0Z2FZVxwOnARcmud25k8wkmU8yv7S0tP/VSpL2qp8R+iJwwrLt47n9lMpzgY0AVfXfSY4E1gE3LG9UVbPALMDU1NTKFwVJgzQ7C3Nzo+1z4U2dP6dfNNp+t2yBmZnR9nkI6meEfhmwIcmJSe5M50PP7SvafBN4PECSBwNHAg7BpXGam4OFhZF2uXPyReycHHGYLyyM/oXrENVzhF5VtybZClwCrAEuqKork5wLzFfVduAlwN8n+Us60zHPripH4NK4TU7Czp3jrmK4pqfHXcEho58pF6pqB7Bjxb6zlz2/CnjMYEuTJO0PV4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRfQV6ko1Jrk6yK8lZe2nzJ0muSnJlkrnBlilJ6mVtrwZJ1gDnAU8EFoHLkmyvqquWtdkA/BXwmKq6Mcl9hlWwJGl1/YzQTwF2VdU1VXULcBFw+oo2zwPOq6obAarqhsGWKUnqpZ9APw64btn2YnffcicBJyX5TJJLk2xc7URJZpLMJ5lfWlo6sIolSavqJ9Czyr5asb0W2ABMA5uBdyc55nZ/qWq2qqaqampiYmJ/a5Uk7UM/gb4InLBs+3jg+lXafKSqfl5V1wJX0wl4SdKI9PxQFLgM2JDkROBbwCZgy4o2H6YzMt+WZB2dKZhrBlnoMMxePsvcFYfmBTkLu98EwPS2F425kr3b8tAtzDxyZtxlSOrqGehVdWuSrcAlwBrggqq6Msm5wHxVbe8ee1KSq4DbgJdV1XeHWfggzF0xx8LuBSaPnRx3KbczedahG+QAC7sXAAx06RDSzwidqtoB7Fix7+xlzwt4cfdxWJk8dpKdz9457jIOO9PbpsddgqQVXCkqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ij+roOXYe2cax43bOwaNTXo7s6Vdo7R+gN2LPidZQmj50c+Qrbhd0Lh+ytGqRDgSP0RtwRVry6OlXaN0foktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhrRV6An2Zjk6iS7kpy1j3ZnJKkkU4MrUZLUj56BnmQNcB5wKnAysDnJyau0uzvwF8DnBl2kJKm3fkbopwC7quqaqroFuAg4fZV2rwZeD/xsgPVJkvrUT6AfB1y3bHuxu+8XkjwCOKGqPrqvEyWZSTKfZH5paWm/i5Uk7V0/gZ5V9tUvDiZ3At4IvKTXiapqtqqmqmpqYmKi/yolST31E+iLwAnLto8Hrl+2fXfgIcDOJF8HHg1s94NRSRqtfgL9MmBDkhOT3BnYBGzfc7CqbqqqdVW1vqrWA5cCT6mq+aFULElaVc9Ar6pbga3AJcCXgYur6sok5yZ5yrALlCT1p69fQVdVO4AdK/advZe20wdfliRpf7lSVJIaYaBLUiMMdElqhIEuSY0w0CWpEX1d5SL1Mnv5LHNXzA21j4XdCwBMb5seaj8AWx66hZlHzgy9H2mQHKFrIOaumPtF4A7L5LGTTB47OdQ+oPPCMewXJ2kYHKFrYCaPnWTns3eOu4yDNop3ANIwOEKXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRXoeupgxixeogVqS60lTj4AhdTRnEitWDXZHqSlONiyN0NWfcK1ZdaapxcYQuSY3oa4SeZCPwZmAN8O6q+tsVx18MnAncCiwBf1ZV3xhwrZION7OzMDfk6aeF7hTb9PRw+wHYsgVmDt3PRnqO0JOsAc4DTgVOBjYnOXlFs88DU1X1MOCDwOsHXaikw9Dc3C8Dd1gmJzuPYVtYGP6L00HqZ4R+CrCrqq4BSHIRcDpw1Z4GVfXJZe0vBZ4xyCIlHcYmJ2HnznFXcfBG8Q7gIPUzh34ccN2y7cXuvr15LvCx1Q4kmUkyn2R+aWmp/yolST31E+hZZV+t2jB5BjAFvGG141U1W1VTVTU1MTHRf5WSpJ76mXJZBE5Ytn08cP3KRkmeAPw18Liqunkw5UmS+tVPoF8GbEhyIvAtYBOwZXmDJI8A3gVsrKobBl7lKlwRKEm/queUS1XdCmwFLgG+DFxcVVcmOTfJU7rN3gAcBXwgyUKS7UOruMsVgZL0q/q6Dr2qdgA7Vuw7e9nzJwy4rr64IlCSfsmVopLUCANdkhrhzbkktW8QtyAYxC0GhnzrAEfokto3iFsQHOwtBkZw6wBH6JLuGMZ9C4IR3DrAEbokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmN6CvQk2xMcnWSXUnOWuX4XZL8U/f455KsH3ShkqR96xnoSdYA5wGnAicDm5OcvKLZc4Ebq+qBwBuB1w26UEnSvvUzQj8F2FVV11TVLcBFwOkr2pwOvKf7/IPA45NkcGVKknpJVe27QXIGsLGqzuxuPxN4VFVtXdbmS902i93tr3XbfGfFuWaAPb8h9UHA1YP6h0jSHcQDqmpitQP9/E7R1UbaK18F+mlDVc0Cs330KUnaT/1MuSwCJyzbPh64fm9tkqwF7gF8bxAFSpL600+gXwZsSHJikjsDm4DtK9psB/60+/wM4BPVay5HkjRQPadcqurWJFuBS4A1wAVVdWWSc4H5qtoOnA9cmGQXnZH5pmEWLUm6vZ4fikqSDg+uFJWkRhjoktQIA12SGnHYBXqSrUnmk9ycZNsY63hwkk8kual7D5unjqmO9Ul2JLkxye4kb+teOjrqOjYl+XKSHyf5WpLHjrj/uyQ5P8k3kvwwyeeTnDrKGrp1vC/Jt5P8IMlXk5w56hq6dexM8rMkP+o+RrqIb1m/ex63JXnrKGtYVsu9kvxL93vzG0m2jKOObi0bul+X9w3j/IddoNO5Bv41wAXjKqAbmB8BPgrci87q1/clOWkM5bwduAG4HzAJPA54wSgLSPJEOvfveQ5wd+D3gWtGWQOdK7auo/PvvwfwSuDiMdwo7rXA+qo6GngK8JokjxxxDXtsraqjuo8HjbLjZf0eBdwX+CnwgVHWsMx5wC3dOp4OvCPJb4+xlsuGdfLDLtCr6kNV9WHgu2Ms47eAXwfeWFW3VdUngM8AzxxDLScCF1fVz6pqN/BxYNTfrH8DnFtVl1bV/1XVt6rqW6MsoKp+XFXnVNXXuzV8FLgWGGmYVtWVVXXzns3u4zdHWcMh6Aw6g47/GnXHSe4GPA14ZVX9qKo+TWfdzMh/VpNsAr4P/Mew+jjsAv0QsdqtDgI8ZNSFAG8GNiW5a5Lj6NwV8+Oj6rx7N84pYKI79bTYnfb5tVHVsJe67gucBFw5hr7fnuQnwFeAbwM7Rl1D12uTfCfJZ5JMj6kG6Cw6fO+YFhueBNxWVV9dtu8LjHjQk+Ro4FzgJcPsx0A/MF+hM+J4WZIjkjyJzlv9u46hlv+k8835Azq3YJgHPjzC/u8LHEFnFPZYOtM+jwBeMcIafkWSI4B/BN5TVV8Zdf9V9QI6U0+PBT4E3LzvvzEULwd+AziOzv2T/jXJyN8pJLk/nZ+N9/RqOyRHATet2HcTna/PKL0aOL+qrhtmJwb6AaiqnwN/DPwhsJvOq+7FdAJ1ZJLcic4K3g8BdwPWAfdktPej/2n3z7dW1be7d9j8O+C0EdbwC93/kwvpzJlu7dF8aLpTcZ+mc++j54+h/89V1Q+r6uaqeg+dKcFxfE2eBXy6qq4dQ98APwKOXrHvaOCHoyogySTwBDq/K2KoDPQDVFVfrKrHVdW9q+rJdEZD/zPiMu5F56Zob+v+4H4X+AdG+INbVTfSeSEb+5Lj7j34z6fzruFp3RfecVvLoTGHXqw+VThsz2J8o3OArwJrk2xYtu/hjHYqbhpYD3wzyW7gpcDTkvzvoDs67AI9ydokR9K5r8yaJEeO6TK9h3X7vmuSl9K5ymTbKGvojoavBZ7f/X85hs585RdGWQedF5EXJrlPknsCL6JzBdCovQN4MPBHVfXTXo0Hrfvv35TkqCRrkjwZ2Ax8YsR1HJPkyXt+NpI8nc6VR5eMuI7fozPlM66rW6iqH9N5B3tukrsleQydX8hz4QjLmKXzoj7ZfbwT+DfgyQPvqaoOqwdwDr+8emDP45wx1PEG4EY6b+k+BjxwTP8fk8DObi3fofPDc58R13AEncsnv09nCuotwJEjruEB3e+Fn3W/JnseTx9hDRN0PtP4Pp3PNK4AnjeG74kJOpfG/bBby6XAE8dQx7uAC0fd7yp13IvO50o/Br4JbBlzPecA7xvGub05lyQ14rCbcpEkrc5Al6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEf8PP+KQHWm0e6YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, ward, single\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = datasets.load_iris().data[:10]\n",
    "\n",
    "linkage_matrix = ward(X)\n",
    "\n",
    "dendrogram(linkage_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Advantage and Disadvantage of HC*\n",
    "    \n",
    "Advantage:\n",
    "1. Hiearchical representations are very informative\n",
    "2. Provide additional visualization\n",
    "\n",
    "Disadvange:\n",
    "1. Sensitive to noises and outliers\n",
    "2. Computational intensive O(N^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Clustering (DBSCAN)\n",
    "\n",
    "It grips the points close to each other.\n",
    "Not every point is identified as clusters. Some of them are identified as noise.\n",
    "\n",
    "1. Starts from a random point.\n",
    "2. Check neighbors with distance $\\epsilon$\n",
    "    - Are there any point?\n",
    "        If no, then this point is taken as noise\n",
    "        If yes, are there greater than MinPts? If no, then noise.\n",
    "3. Identify the core point and clusters\n",
    "4. check other points in this cluster whether they are border point or core point\n",
    "5. The cluster becomes larger with new core points found in the same cluster.\n",
    "\n",
    "DBSCAN:\n",
    "    1. two crescents\n",
    "    2. two rings\n",
    "    3. elongated shapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "       metric_params=None, min_samples=5, n_jobs=None, p=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, cluster\n",
    "\n",
    "X = datasets.load_iris().data\n",
    "\n",
    "db = cluster.DBSCAN(eps = 0.5, min_samples = 5)\n",
    "\n",
    "db.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "    1. No need to specify the number of clusters\n",
    "    2. Flexible shapes\n",
    "    3. It can deal with noise and outliers\n",
    "    \n",
    "Disadvantage: \n",
    "    1. Border points taht are reachable from two clusters\n",
    "    2. Faces difficulty finding clusters of varying densities.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 Gaussian Mixture model clustering\n",
    "\n",
    "Every points will belong to one of the clusters but have different level of membership.\n",
    "\n",
    "It works really well on the data sets having elongated shapes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM) Clustering\n",
    "\n",
    "The clusters are described by different gaussian distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization (EM) Algorithm\n",
    "\n",
    "1. Initialize K Gaussian distribution\n",
    "    - Give initial mean and standard deviation\n",
    "          a. estimate the mean and std for all data\n",
    "          b. run K-means and initialize the mean and standard deviation\n",
    "2. Soft-cluster data \"expectation\"\n",
    "    - pdf(cluster1)/(pdf(cluster1) + pdf(cluster2))\n",
    "   \n",
    "3. re-estimate the gaussians \"maximization\"\n",
    "    - use weighted average of soft-cluster mean\n",
    "    - use weighted variance of soft-cluster variance\n",
    "4. evaluate the log-likelihood to check for the convergence\n",
    "    -how to choose the mixing coefficient??\n",
    "5. Go back to step 2 if not converging yet.\n",
    "\n",
    "**Covariance-type** also affect the shape of the clusters. Use covariance matrix to do this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, mixture\n",
    "X = datasets.load_iris().data[:10]\n",
    "gmm = mixture.GaussianMixture(n_components = 3)\n",
    "gmm.fit(X)\n",
    "\n",
    "clustering = gmm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Adavantages:\n",
    "    1. Soft-clustring (sample membership)\n",
    "    2. Flexible cluster shape\n",
    "Disadvantages:\n",
    "    1. Sensitive to initial values\n",
    "    2. Converge slow and may end up with local optimal solution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "\n",
    "1. Data\n",
    "2. Feature selection; Feature extraction\n",
    "3. Clustering algorithm and tuning\n",
    "4. Cluster Validation\n",
    "5. Results interpretation\n",
    "6. Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster validation\n",
    "\n",
    "Categories of cluster validation indices:\n",
    "\n",
    "1. External indices: if the data is originally labeled.\n",
    "2. Internal indices: No labels\n",
    "3. Relative indices\n",
    "\n",
    "Indices are defined by compactness and separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External indices\n",
    "External indices require labels.\n",
    "Let C indicate the clusters with true labels and K indicate the clusters in the result.\n",
    "\n",
    "1. Adjusted rand score (sklearn) [-1,1]\n",
    "    $$\n",
    "    Rand Index = \\frac{a+b}{\\binom n 2}\n",
    "    $$\n",
    "  Random assignment: ARI close to 0\n",
    "  Good clustering: ARI close to 1\n",
    "a is the number of pairs in the same cluster C remains in the same cluster in K\n",
    "b is the number of pairs in different clusters in C remains in different clusters in K.\n",
    "2. Fawlks and Mallows (sklearn)\n",
    "3. NMI measure (sklearn)\n",
    "4. Jaccard (sklearn)\n",
    "5. F-measure (sklearn)\n",
    "6. Purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal indices\n",
    "\n",
    "1. Silhouette index (sklearn) [-1,1]\n",
    "    $$\n",
    "    \\text{silhouette coefficient }= \\frac{b_i - a_i}{\\max (a_i, b_i)}\n",
    "    $$\n",
    "    $a_i$ is the average distance to other samples in the same clsuter\n",
    "    $b_i$ is the average distance to samples in the closest neighboring cluster\n",
    "    $$\n",
    "    S = average(S_1, S_2, ....)\n",
    "    $$\n",
    "    \n",
    "**When to use?**\n",
    "1. S vs number of clusters\n",
    "2. S vs clustering algorithm\n",
    "\n",
    "    **Silhoutte score should not** be used in DBSCAN, because this score does not have the concept of noise.\n",
    "\n",
    "    **If the dataset is two rings**, silhoutte score should not be used, because the score is built to carving out more compact and dense clusters.\n",
    "    \n",
    "2. Calinski-Harabasz (sklearn)\n",
    "\n",
    "3. BIC\n",
    "\n",
    "4. Dunn Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 Dimension reduction and Principle component analysis (PCA)\n",
    "\n",
    "1. Dimension reduction through feature selection/extraction\n",
    "2. PCA\n",
    "3. Fitting\n",
    "4. Interpret\n",
    "\n",
    "**Feature selection**\n",
    "\n",
    "Feature selection is to find a subset of original features.\n",
    "\n",
    "1. Filter methods\n",
    "    This method filters out features with weak correlations according to  Pearson's correlation, Linear Discriminant Analysis (LDA) and Analysis of Variance (ANOVA)\n",
    "\n",
    "    LASSO regression method in supervised learning embeds the property of feature selection.\n",
    "\n",
    "2. Wrapper methods\n",
    "    This selects features by cross-validating algorithm with different subsets of features.\n",
    "    It includes: Forward Search, Backward Search, Recursive Feature Elimination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extraction**\n",
    "\n",
    "It creates new *latent features* combining the information of the orinigal features.\n",
    "\n",
    "1. PCA\n",
    "2. Independent Component Analysis (ICA)\n",
    "3. Random Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA properties\n",
    "\n",
    "1. Each component captures the largest amount of variance left in the data\n",
    "2. Components are orthogonal to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5  Random Projection and ICA\n",
    "\n",
    "## Random Projection\n",
    "Random projection is more computationally efficient than PCA.\n",
    "\n",
    "Multiply the dataset by a random matrix which projects the dataset from d dimension to a lower k dimension.\n",
    "\n",
    "This is based on *Johnson-linderstrauss lemma:*\n",
    "    \n",
    "    \n",
    "    projection in a way that preserves the distrance between the points to a large degree.\n",
    "\n",
    "\n",
    "How to preserve?\n",
    "\n",
    "    The distance between the projected points wil be in an interval as follows, \n",
    "\n",
    "$$\n",
    "(1-\\epsilon) ||u - v||^2, (1+\\epsilon)||u-v||^2\n",
    "$$\n",
    "\n",
    "*The number of components is optional and can be computed by the algorithm based on the given $\\epsilon$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Projection in skleanr**\n",
    "\n",
    "```\n",
    "from sklearn import random_projection\n",
    "\n",
    "rp = random_projection.SparseRandomProjection()\n",
    "new_X = rp.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Component Analysis\n",
    "\n",
    "ICA assumes the data is the mixture of several independent sources\n",
    "and it tries to isolate these sources.\n",
    "\n",
    "ICA solves a kind of problem which is called blind-source separation.\n",
    "\n",
    "Let $X$ denote the data, $A$ denote the mixing matrix, $S$ the source data and $W$ denote the inverse of $A$.\n",
    "\n",
    "Then ICA algorithm solves the inverse of the matrix A to separate the original source S.\n",
    "$$\n",
    "S = W X\n",
    "$$\n",
    "\n",
    "*Assumptions of ICA:*\n",
    "    \n",
    "1. Components are independent\n",
    "2. Components were non-Gaussian \n",
    "3. CLT convergence to Gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components = 3)\n",
    "components= ica.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
